import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report
from imblearn.over_sampling import ADASYN
from sklearn.feature_selection import SelectFromModel
import joblib
import os
from sklearn.feature_extraction.text import TfidfVectorizer

# Load datasets
print("Loading datasets...")
train_data_path = './TrainingDatasets2/train_malicious.csv'
test_data_path = './TrainingDatasets2/train_benign.csv'
train_data = pd.read_csv(train_data_path)
test_data = pd.read_csv(test_data_path)

# Handle missing data
train_data.fillna(0, inplace=True)
test_data.fillna(0, inplace=True)

# Combine datasets
combined_data = pd.concat([train_data, test_data], ignore_index=True)
print("Datasets combined.")

# Preprocess text columns
text_columns = ['detected_keywords', 'encoded_strings', 'complex_patterns']
combined_text = combined_data[text_columns].fillna('').astype(str).apply(lambda x: ' '.join(x), axis=1)
vectorizer = TfidfVectorizer(max_features=1000)
text_features = vectorizer.fit_transform(combined_text)
print("Text data vectorized.")

# Replace text columns in dataframe with vectorized text
text_features_df = pd.DataFrame(text_features.toarray(), columns=vectorizer.get_feature_names_out())
combined_data = pd.concat([combined_data.drop(columns=text_columns), text_features_df], axis=1)

# Separate features and target
features = combined_data.drop(columns=['Class', 'name'])
target = combined_data['Class']
print("Features and target separated.")

# Balance classes with ADASYN
adasyn = ADASYN(random_state=42)
features_resampled, target_resampled = adasyn.fit_resample(features, target)
print("Classes balanced using ADASYN.")

# Feature selection
feature_selector = RandomForestClassifier(random_state=42)
feature_selector.fit(features_resampled, target_resampled)
selected_features_model = SelectFromModel(feature_selector, prefit=True)
features_resampled = selected_features_model.transform(features_resampled)
features = selected_features_model.transform(features)  # Ensure test features are aligned
print(f"Selected features shape: {features_resampled.shape}")

# Scale features
scaler = StandardScaler()
features_resampled = scaler.fit_transform(features_resampled)
features_scaled = scaler.transform(features)  # Apply the same scaler to test data
print("Features scaled.")

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(features_resampled, target_resampled, test_size=0.2, random_state=42, stratify=target_resampled)
print("Data split into training and test sets.")

# Train models
print("Training models...")
rf_model = RandomForestClassifier(random_state=42)
gb_model = HistGradientBoostingClassifier(random_state=42)
svc_model = SVC(probability=True, random_state=42)

# Hyperparameter tuning for Gradient Boosting
param_grid = {
    'max_depth': [3, 5, 10],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_iter': [100, 200, 300]
}
gb_search = RandomizedSearchCV(gb_model, param_grid, n_iter=10, cv=5, random_state=42)
gb_search.fit(X_train, y_train)
print(f"Best Gradient Boosting Parameters: {gb_search.best_params_}")
gb_model = gb_search.best_estimator_

# Voting classifier
voting_model = VotingClassifier(estimators=[('rf', rf_model), ('gb', gb_model), ('svc', svc_model)], voting='soft')
voting_model.fit(X_train, y_train)
print("Models trained and ensemble model created.")

# Cross-validation
cv_scores = cross_val_score(voting_model, X_train, y_train, cv=5, scoring='f1_weighted')
print(f"Cross-Validation F1-Score: {cv_scores.mean():.2f} Â± {cv_scores.std():.2f}")

# Save models and preprocessing tools
model_save_path = 'PDFMalware/models16/secondary_csv/'
os.makedirs(model_save_path, exist_ok=True)
joblib.dump(voting_model, os.path.join(model_save_path, 'voting_model.pkl'))
joblib.dump(vectorizer, os.path.join(model_save_path, 'vectorizer.pkl'))
joblib.dump(scaler, os.path.join(model_save_path, 'scaler.pkl'))
joblib.dump(selected_features_model, os.path.join(model_save_path, 'feature_selector.pkl'))
print("Models and preprocessing tools saved.")

# Evaluate model
y_pred = voting_model.predict(X_test)
print("Evaluating the ensemble model...")
print("Classification Report:\n", classification_report(y_test, y_pred))

