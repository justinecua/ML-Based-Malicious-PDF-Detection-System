import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, classification_report
from imblearn.over_sampling import SMOTE  # Import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

# Define the filename
filename = 'PDFMalware2022.csv'

# Load the dataset
df = pd.read_csv(filename)

# Display basic information about the dataset
print(df.info())
print(df.describe())

# Handle missing values only for numeric columns
numeric_cols = df.select_dtypes(include=[np.number]).columns
df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())  # Fill missing values with column mean

# Check unique values in the 'Class' column
print("Unique values in 'Class':", df['Class'].unique())

# Filter out any class labeled as 2
df = df[df['Class'].isin([0, 1])]

# Convert 'Class' to a uniform type (string)
df['Class'] = df['Class'].astype(str)

# Encode target variable
le = LabelEncoder()
df['Class'] = le.fit_transform(df['Class'])

# Drop non-numeric columns for feature scaling, keeping only numeric columns
df_cleaned = df.drop(columns=['header', 'Fine name', 'Class'])  # Drop 'header' and 'Fine name'

# Convert all columns to numeric where possible, errors='coerce' will turn non-convertible values to NaN
df_cleaned = df_cleaned.apply(pd.to_numeric, errors='coerce')

# Handle remaining NaN values after conversion (if any)
df_cleaned.fillna(df_cleaned.mean(), inplace=True)

# Split features and target
features = df_cleaned
target = df['Class']

# Scale features
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)

# Apply SMOTE to the training data
smote = SMOTE()
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Initialize models with class weights
rf_model = RandomForestClassifier(class_weight='balanced')
svm_model = SVC(probability=True, class_weight='balanced')
gb_model = GradientBoostingClassifier()  # No built-in class weights, SMOTE will handle imbalance

# Train models
rf_model.fit(X_resampled, y_resampled)
svm_model.fit(X_resampled, y_resampled)
gb_model.fit(X_resampled, y_resampled)

# Make predictions
rf_preds = rf_model.predict(X_test)
svm_preds = svm_model.predict(X_test)
gb_preds = gb_model.predict(X_test)

# Evaluate models
print("Random Forest Accuracy:", accuracy_score(y_test, rf_preds))
print("SVM Accuracy:", accuracy_score(y_test, svm_preds))
print("Gradient Boosting Accuracy:", accuracy_score(y_test, gb_preds))

print("Random Forest Precision:", precision_score(y_test, rf_preds, average='weighted'))
print("SVM Precision:", precision_score(y_test, svm_preds, average='weighted'))
print("Gradient Boosting Precision:", precision_score(y_test, gb_preds, average='weighted'))

# Get probability predictions
rf_probs = rf_model.predict_proba(X_test)[:, 1]
svm_probs = svm_model.predict_proba(X_test)[:, 1]
gb_probs = gb_model.predict_proba(X_test)[:, 1]

# Calculate weights based on accuracy
rf_weight = accuracy_score(y_test, rf_preds) / (accuracy_score(y_test, rf_preds) + accuracy_score(y_test, svm_preds) + accuracy_score(y_test, gb_preds))
svm_weight = accuracy_score(y_test, svm_preds) / (accuracy_score(y_test, rf_preds) + accuracy_score(y_test, svm_preds) + accuracy_score(y_test, gb_preds))
gb_weight = accuracy_score(y_test, gb_preds) / (accuracy_score(y_test, rf_preds) + accuracy_score(y_test, svm_preds) + accuracy_score(y_test, gb_preds))

# Weighted average probabilities
ensemble_probs = (rf_weight * rf_probs) + (svm_weight * svm_probs) + (gb_weight * gb_probs)

# Final predictions
final_predictions = np.where(ensemble_probs > 0.5, 1, 0)  # Adjust the threshold as needed

# Display classification report
print(classification_report(y_test, final_predictions))

# Optionally plot confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(confusion_matrix(y_test, final_predictions), annot=True, fmt='d')
plt.title("Confusion Matrix for Ensemble Model")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Save models
joblib.dump(rf_model, 'PDFMalware/models/rf_model.pkl')
joblib.dump(svm_model, 'PDFMalware/models/svm_model.pkl')
joblib.dump(gb_model, 'PDFMalware/models/gb_model.pkl')
joblib.dump(scaler, 'PDFMalware/models/scaler.pkl')
joblib.dump(le, 'PDFMalware/models/label_encoder.pkl')

